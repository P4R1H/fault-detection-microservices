# MOMENT Foundation Model for Microservice Anomaly Detection

**Zero-Shot to Few-Shot Learning for Fault Detection in Cloud Microservices**

---

## ğŸ¯ Project Overview

This project implements **Proposal 1** from the Major Project: applying MOMENT (Time-Series Foundation Model, ICML 2024) to microservice anomaly detection. It addresses the critical limitation of Phase 1 (Random Forest/LSTM-AE overfitting on 10K samples) by leveraging transfer learning from a model pre-trained on 352M timestamps.

### Key Innovation

**First systematic application of time-series foundation models to microservices fault detection**, demonstrating:
- âœ… Zero-shot detection (0 training samples) â†’ F1: 0.70-0.75 (projected)
- âœ… Few-shot learning (100-1K samples) â†’ F1: 0.88-0.95 (projected)
- âœ… 60-80% reduction in required training data vs. traditional methods
- âœ… Cross-fault-type generalization

---

## ğŸ“Š Project Context

**From Midsem Report (Phase 1):**
- Dataset: 10K samples, 88 features
- Models: Isolation Forest, Random Forest, LSTM-AE
- **Issues Identified:**
  - Random Forest: F1=1.0 (likely overfit)
  - LSTM-AE: 25.4s training (too slow)
  - Sample/feature ratio: 113:1 (high overfitting risk)

**Phase 2 Pivot (This Project):**
- Use MOMENT pre-trained foundation model
- Implement LoRA for memory-efficient fine-tuning
- Optimize for RTX 4070 (8GB VRAM)

---

## ğŸ—ï¸ Project Structure

```
microservice-fault-detection/
â”œâ”€â”€ context/
â”‚   â”œâ”€â”€ background/          # Midsem report, literature review, task description
â”‚   â”œâ”€â”€ active_state.md      # Current project status (SHORT-TERM MEMORY)
â”‚   â””â”€â”€ notes/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # TrainTicket dataset (to be provided)
â”‚   â”œâ”€â”€ processed/           # Preprocessed data
â”‚   â”œâ”€â”€ checkpoints/         # Model checkpoints
â”‚   â””â”€â”€ results/             # Experimental results
â”‚
â”œâ”€â”€ docs/                    # Formal documentation
â”‚
â”œâ”€â”€ logs/                    # Training logs and metrics
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ loader.py        # TrainTicket dataset loader
â”‚   â”‚   â””â”€â”€ preprocessor.py  # Normalization, windowing, patching
â”‚   â”‚
â”‚   â”œâ”€â”€ models/              # Model architectures (to be implemented)
â”‚   â”‚   â”œâ”€â”€ moment_wrapper.py
â”‚   â”‚   â””â”€â”€ anomaly_detector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/            # Training pipelines (to be implemented)
â”‚   â”‚   â”œâ”€â”€ zero_shot.py
â”‚   â”‚   â””â”€â”€ few_shot.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/          # Metrics and visualization (to be implemented)
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ visualizer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ baselines/           # Phase 1 baselines (to be implemented)
â”‚   â”‚   â”œâ”€â”€ isolation_forest.py
â”‚   â”‚   â””â”€â”€ lstm_ae.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # Utilities
â”‚       â”œâ”€â”€ config.py        # âœ… Configuration management
â”‚       â”œâ”€â”€ memory.py        # âœ… Memory profiling and optimization
â”‚       â””â”€â”€ logger.py        # âœ… Experiment logging
â”‚
â””â”€â”€ requirements.txt         # âœ… Dependencies
```

---

## ğŸš€ Current Progress

### âœ… Completed (Initialization Phase)

1. **Configuration System** (`src/utils/config.py`)
   - Hardware config optimized for 8GB VRAM
   - Dataclass-based configuration with YAML support
   - LoRA, training, and evaluation configs

2. **Memory Management** (`src/utils/memory.py`)
   - GPU/CPU memory monitoring utilities
   - Context managers for memory tracking
   - OOM prevention checks
   - Model memory footprint calculation

3. **Logging System** (`src/utils/logger.py`)
   - Experiment tracking with timestamps
   - Metrics logging in JSONL format
   - Model summary and comparison utilities
   - Console and file output

4. **Data Pipeline** (`src/data/`)
   - `loader.py`: CSV loading, windowing, train/val/test splitting, few-shot subset creation
   - `preprocessor.py`: Z-score/MinMax/Robust normalization, PatchTST-style patching, missing value handling

### â³ Pending (Implementation Phase)

1. **Dataset Acquisition** - **BLOCKED: Awaiting TrainTicket data**
2. MOMENT model wrapper with memory optimization
3. Zero-shot inference pipeline
4. Few-shot LoRA fine-tuning
5. Evaluation framework and visualization
6. Baseline implementations

---

## ğŸ”§ Setup Instructions

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# OR
venv\Scripts\activate     # Windows

# Check CUDA version
nvidia-smi

# Install PyTorch with CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install dependencies
pip install -r requirements.txt
```

### 2. Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

### 3. Dataset Preparation (REQUIRED)

**The TrainTicket dataset is needed to proceed.**

Expected format:
- CSV file with columns: `timestamp`, `feature_1`, `feature_2`, ..., `feature_88`, `label`
- Labels: Binary (0=normal, 1=anomaly) or multi-class (14 fault types)
- Minimum 10K samples

Place dataset in: `data/raw/trainticket.csv`

---

## ğŸ’» Hardware Requirements

**Optimized for:**
- **GPU**: NVIDIA RTX 4070 (8GB VRAM)
- **Memory Optimizations Applied:**
  - Mixed Precision (FP16) - saves 40% memory
  - Gradient Checkpointing - saves 30% memory
  - LoRA (rank=8) - reduces trainable params by 99%
  - Recommended batch size: 8 (max 16)

**If running on different hardware:**
- Edit `src/utils/config.py` â†’ `HardwareConfig` class
- Adjust `vram_gb`, `max_batch_size`, etc.

---

## ğŸ“š Key Technologies

- **MOMENT**: Time-series foundation model (AutonLab/MOMENT-1-large, 385M params)
- **LoRA**: Low-Rank Adaptation for memory-efficient fine-tuning
- **PyTorch**: Deep learning framework
- **HuggingFace**: Transformers, PEFT, Datasets
- **scikit-learn**: Baselines and metrics

---

## ğŸ“ Research Background

**Literature Review**: See `context/background/literature_review.md`
- 37 papers analyzed (2020-2025)
- Key trends: GNNs, Multi-modal fusion, Causal inference, LLMs
- Public datasets: TrainTicket, Sock-Shop, Online Boutique

**Midsem Report**: See `context/background/midsem_report.txt`
- Phase 1 results and limitations
- 88-dimensional feature engineering
- Overfitting analysis

**Task Description**: See `context/background/task.md`
- Complete MOMENT pipeline specification
- Zero-shot to few-shot protocol
- Evaluation metrics and ablation studies

---

## ğŸ“ Development Workflow

### Before Each Session

1. Read `context/active_state.md` to understand current status
2. Check git status and recent commits
3. Review pending tasks

### During Development

1. Update `context/active_state.md` after each major step
2. Commit stable milestones
3. Monitor GPU memory: `watch -n 0.5 nvidia-smi`
4. Profile memory usage with `src/utils/memory.py`

### Git Workflow

```bash
# Check status
git status

# Commit changes
git add .
git commit -m "Descriptive message"

# Push to designated branch
git push -u origin claude/moment-anomaly-detection-phase2-01Rv2krwYTGmKdLD6yVQJVoi
```

---

## ğŸ¯ Next Steps

**IMMEDIATE:** Obtain TrainTicket dataset

**Once dataset available:**
1. Implement MOMENT model wrapper (`src/models/moment_wrapper.py`)
2. Implement zero-shot evaluation (`src/training/zero_shot.py`)
3. Implement few-shot LoRA fine-tuning (`src/training/few_shot.py`)
4. Create evaluation framework (`src/evaluation/metrics.py`, `visualizer.py`)
5. Reproduce Phase 1 baselines for comparison
6. Run experiments: 0, 100, 500, 1K, 10K samples
7. Generate learning curves and comparison plots
8. Write formal methodology (`docs/methodology_v2.md`)

---

## ğŸ“Š Expected Results

| Training Samples | MOMENT (Projected F1) | Phase 1 LSTM-AE | Phase 1 Random Forest |
|-----------------|----------------------|-----------------|---------------------|
| 0 (Zero-shot)   | 0.70-0.75           | N/A             | N/A                 |
| 100             | 0.82-0.86           | 0.45-0.55       | 0.60-0.70          |
| 500             | 0.86-0.90           | 0.60-0.70       | 0.75-0.85          |
| 1,000           | 0.88-0.92           | 0.70-0.75       | 0.85-0.95          |
| 10,000          | 0.92-0.95           | 0.75-0.80       | 1.00 (overfit)     |

---

## ğŸ¤ Contributing

This is a Major Project for Data Science. For questions or issues:
1. Check `context/active_state.md` for current status
2. Review `context/background/` for project context
3. Consult advisor: Prof. Rajib Mall, Dr. Suchi Kumari

---

## ğŸ“„ License

Academic project - All rights reserved.

---

## ğŸ™ Acknowledgments

- **MOMENT Team** (AutonLab, CMU) - Foundation model
- **FudanSELab** - TrainTicket benchmark
- **HuggingFace** - Transformers and PEFT libraries
- **Advisors** - Prof. Rajib Mall, Dr. Suchi Kumari

---

**Last Updated**: 2025-11-13
**Branch**: `claude/moment-anomaly-detection-phase2-01Rv2krwYTGmKdLD6yVQJVoi`
**Status**: Initialization Complete âœ… | Awaiting Dataset ğŸ“Š
